use crate::{
    Dtype, DtypeVal,
    ops::Op,
    tpy::{self},
    trs::{Storage, Tensor, ViewOpError},
};
use std::{
    cell::RefCell,
    cmp,
    ops::{Add, AddAssign, Div, Mul, Neg, Sub},
    rc::Rc,
};
use thiserror::Error;

#[derive(Error, Debug)]
pub enum OpForwardError {
    #[error(transparent)]
    TensorError(#[from] ViewOpError),
    #[error("unknown operation error")]
    Unknown,
}

pub fn forward_cpu(op: &Op) -> Result<Tensor, OpForwardError> {
    match op {
        Op::Add(x, y) => zip_cpu(|xi, yi| xi + yi, x, y),
        Op::Sub(x, y) => zip_cpu(|xi, yi| xi - yi, x, y),
        Op::Mul(x, y) => zip_cpu(|xi, yi| xi * yi, x, y),
        Op::Div(x, y) => zip_cpu(|xi, yi| xi / yi, x, y),
        Op::Neg(x) => map_cpu(&op, |xi| -xi, x),
        // TODO: generalize transcental uops to other dtypes?
        Op::Exp(x) => map_cpu(&op, |xi| DtypeVal::Float32(f32::from(xi).exp()), x),
        Op::Log(x) => map_cpu(&op, |xi| DtypeVal::Float32(f32::from(xi).ln()), x),
        Op::Sinh(x) => map_cpu(&op, |xi| DtypeVal::Float32(f32::from(xi).sinh()), x),
        Op::Cosh(x) => map_cpu(&op, |xi| DtypeVal::Float32(f32::from(xi).cosh()), x),
        Op::Tanh(x) => map_cpu(&op, |xi| DtypeVal::Float32(f32::from(xi).tanh()), x),
        Op::Sum(x, dim, keepdim) => reduce_cpu(|xi, yi| xi + yi, x, *dim, *keepdim),
        Op::Max(x, dim, keepdim) => reduce_cpu(
            // TODO: maybe_reduce_cpu where F: DTypeVal, DTypeVal -> Option<DTypeVal>?
            |xi, yi| match xi.partial_cmp(&yi).unwrap() {
                cmp::Ordering::Less => yi,
                cmp::Ordering::Equal => xi,
                cmp::Ordering::Greater => xi,
            },
            x,
            *dim,
            *keepdim,
        ),
        Op::Matmul(X, Y) => matmul_cpu(X, Y),
    }
}

fn map_cpu<F>(op: &Op, f: F, x: &Tensor) -> Result<Tensor, OpForwardError>
where
    F: Fn(DtypeVal) -> DtypeVal,
{
    Ok(Tensor {
        ndim: x.ndim,
        shape: x.shape.clone(),
        stride: x.stride.clone(),
        input_op: Some(Box::new(op.clone())),
        requires_grad: x.requires_grad,
        storage: Rc::new(RefCell::new(Storage {
            data: x.storage.borrow().data.iter().map(|&xi| f(xi)).collect(),
            grad: None,
        })),
        device: x.device.clone(),
        layout: x.layout.clone(),
        dtype: x.dtype.clone(),
    })
}

// 1. z_shape <- broadcast_shape(x.shape, y.shape)
// 2. for physz in z.numel():

//      **a. find the logical nd index of x and y**
//      3. logz <- encode(i) # lift physical z to logical z
//      4. (logx, logy) <- broadcast_logidx(logx, logz), broadcast_logidx(logy, logz) # map logical z over to logical x and y
//      5. physx, physy <- decode(logx), decode(logy) # lower x and y to physical

//      6. z[physz] <- f(x[physx], y[physy])
fn zip_cpu<F>(f: F, x: &Tensor, y: &Tensor) -> Result<Tensor, OpForwardError>
where
    F: Fn(DtypeVal, DtypeVal) -> DtypeVal,
{
    let (x, y, z) = if x.shape != y.shape {
        let z_shape = Tensor::broadcast_shape(&x.shape, &y.shape)?;
        let z = tpy::zeros(z_shape, Dtype::Float32); // clone because of python/rust memory mismatch

        let (x_stride, y_stride) = (
            Tensor::clamp_stride(&x.shape),
            Tensor::clamp_stride(&y.shape),
        );

        let (mut x, mut y) = (x.clone(), y.clone());
        x.stride = x_stride;
        y.stride = y_stride;

        (x, y, z)
    } else {
        let z = tpy::zeros(x.shape.clone(), Dtype::Float32); // clone because of python/rust memory mismatch
        (x.clone(), y.clone(), z) // TODO: possibly remove taking view just to satisfy typechecker
    };

    // println!("x.shape: {:?}, x.stride: {:?}", x.shape, x.stride);
    // println!("y.shape: {:?}, y.stride: {:?}", y.shape, y.stride);
    // println!("z.shape: {:?}, z.stride: {:?}", z.shape, z.stride);

    {
        let (x_storage, y_storage, mut z_storage) = (
            x.storage.borrow(),
            y.storage.borrow(),
            z.storage.borrow_mut(),
        );
        let (logx, logy) = (vec![0; x.ndim], vec![0; y.ndim]);

        for phyz in 0..z.numel() {
            let logz = Tensor::encode(phyz, &z.shape);

            let (logx, logy) = (
                Tensor::broadcast_logidx(&logx, &logz)?,
                Tensor::broadcast_logidx(&logy, &logz)?,
            );
            let (phyx, phyy) = (
                Tensor::decode(&logx, &x.stride),
                Tensor::decode(&logy, &y.stride),
            );

            // println!("phy: {:?}", phyz);
            // println!("logx {:?} -> physx: {:?}", logx, phyx);
            // println!("logy {:?} -> physy: {:?}", logy, phyy);
            // println!("logz {:?} -> physz: {:?}", logz, phyz);

            z_storage.data[phyz] = f(x_storage.data[phyx], y_storage.data[phyy]);
        }
    }

    Ok(z)
}

fn reduce_cpu<F>(
    f: F,
    x: &Tensor,
    dim_reduce: usize,
    keepdim: bool,
) -> Result<Tensor, OpForwardError>
where
    F: Fn(DtypeVal, DtypeVal) -> DtypeVal,
{
    let y_shape = x
        .shape
        .iter()
        .enumerate()
        .map(|(i, &dim_size)| if i == dim_reduce { 1 } else { dim_size })
        .collect();
    let mut y = tpy::zeros(y_shape, Dtype::Float32); // clone because of python/rust memory mismatch

    {
        let (x_storage, mut y_storage) = (x.storage.borrow(), y.storage.borrow_mut());
        for physy in 0..y.numel() {
            let mut logy = Tensor::encode(physy, &y.shape);

            for d in 0..x.shape[dim_reduce] {
                logy[dim_reduce] = d;
                let physx = Tensor::decode(&logy, &x.stride);

                // accumulate/reduce
                y_storage.data[physy] = f(y_storage.data[physy], x_storage.data[physx]);
            }
        }
    }

    if !keepdim {
        y.shape.remove(dim_reduce);
        y.ndim -= 1;
    }

    Ok(y)
}

fn matmul_cpu(X: &Tensor, Y: &Tensor) -> Result<Tensor, OpForwardError> {
    // 1. def O(n^3)
    // 2. data oriented(cache)/pthreads/SIMD
    let (N, M1, M2, P) = (X.shape[0], X.shape[1], Y.shape[0], Y.shape[1]);
    assert_eq!(M1, M2, "Shape mismatch in operation");
    assert_eq!(X.ndim, 2, "X must be a 2D tensor");
    assert_eq!(Y.ndim, 2, "Y must be a 2D tensor");

    let Z = tpy::zeros(vec![N, P], Dtype::Float32);

    {
        let (X_storage, Y_storage, mut Z_storage) = (
            X.storage.borrow(),
            Y.storage.borrow(),
            Z.storage.borrow_mut(),
        );

        // p linear combinations of m basis vectors in R^n (linear combination p times)

        // (nxm)@(m@p)
        for n in 0..N {
            for p in 0..P {
                // NB: p linear combinations of m2 entries mapped to m1 basis vectors in R^n
                // the loops over n p are interchangeable (n*p or p*n).
                // wikpedia has n*p even though p*n is more geometically intuitive

                // dot product of m entries mapped to m basis vectors
                for m in 0..M1 {
                    // dot product:
                    let x = X_storage.data[n * X.stride[0] + m * X.stride[1]];
                    let y = Y_storage.data[m * Y.stride[0] + p * Y.stride[1]];
                    Z_storage.data[n * Z.stride[0] + p * Z.stride[1]] += x * y;
                }
            }
        }
    }

    Ok(Z)
}

impl Add for DtypeVal {
    type Output = DtypeVal;

    fn add(self, other: Self) -> Self::Output {
        match (self, other) {
            (DtypeVal::Float32(x), DtypeVal::Float32(y)) => DtypeVal::Float32(x + y),
            (DtypeVal::Int32(x), DtypeVal::Int32(y)) => DtypeVal::Int32(x + y),
            _ => todo!(),
        }
    }
}

impl AddAssign for DtypeVal {
    fn add_assign(&mut self, other: Self) {
        *self = self.add(other);
    }
}

impl Sub for DtypeVal {
    type Output = DtypeVal;

    fn sub(self, other: Self) -> Self::Output {
        match (self, other) {
            (DtypeVal::Float32(x), DtypeVal::Float32(y)) => DtypeVal::Float32(x - y),
            (DtypeVal::Int32(x), DtypeVal::Int32(y)) => DtypeVal::Int32(x - y),
            _ => todo!(),
        }
    }
}

impl Mul for DtypeVal {
    type Output = DtypeVal;

    fn mul(self, other: Self) -> Self::Output {
        match (self, other) {
            (DtypeVal::Float32(x), DtypeVal::Float32(y)) => DtypeVal::Float32(x * y),
            (DtypeVal::Int32(x), DtypeVal::Int32(y)) => DtypeVal::Int32(x * y),
            _ => todo!(),
        }
    }
}

impl Div for DtypeVal {
    type Output = DtypeVal;

    fn div(self, other: Self) -> Self::Output {
        match (self, other) {
            (DtypeVal::Float32(x), DtypeVal::Float32(y)) => DtypeVal::Float32(x / y),
            (DtypeVal::Int32(x), DtypeVal::Int32(y)) => DtypeVal::Int32(x / y),
            _ => todo!(),
        }
    }
}

impl Neg for DtypeVal {
    type Output = DtypeVal;

    fn neg(self) -> Self::Output {
        match self {
            DtypeVal::Float32(x) => DtypeVal::Float32(-x),
            DtypeVal::Int32(x) => DtypeVal::Int32(-x),
            _ => todo!(),
        }
    }
}
